{"file":{"path":"E:\\Noah4\\PDF1\\FNN\\6788-revisit-fuzzy-neural-network-demystifying-batch-normalization-and-relu-with-generalized-hamming-network.pdf","name":"6788-revisit-fuzzy-neural-network-demystifying-batch-normalization-and-relu-with-generalized-hamming-network.pdf","ext":".pdf","length":3644765,"dir":"E:\\Noah4\\PDF1\\FNN","creation":"2021-05-12T14:37:08","lastaccess":"2021-05-12T14:38:06","lastwrite":"2018-02-19T16:45:23"},"hash":{"md5":"B59712904CF73B024A3FE0D20F2B3A12","sha1":"87CE33C08340F771DE3ED4D687622CDB049F1196"},"tika":{"book":"Advances in Neural Information Processing Systems 30","content-type":"application/pdf","created":"2017","description":"Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)","description-abstract":"We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the “normalized” bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.","editors":"I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","eventtype":"Poster","last-modified":"2018-02-13T09:03:50Z","last-save-date":"2018-02-13T09:03:50Z","published":"2017","publisher":"Curran Associates, Inc.","type":"Conference Proceedings","x-parsed-by":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"access_permission.assemble_document":true,"access_permission.can_modify":true,"access_permission.can_print":true,"access_permission.can_print_degraded":true,"access_permission.extract_content":true,"access_permission.extract_for_accessibility":true,"access_permission.fill_in_form":true,"access_permission.modify_annotations":true,"cp.subject":"Neural Information Processing Systems http://nips.cc/","date":"2018-02-13T09:03:50Z","dc.format":"application/pdf; version=1.3","dcterms.modified":"2018-02-13T09:03:50Z","firstpage":"1923","lastpage":"1932","meta.save-date":"2018-02-13T09:03:50Z","modified":"2018-02-13T09:03:50Z","pdf.pdfversion":"1.3","pdf.charsperpage":["2825","3855","2480","2648","3345","3291","3426","2707","2896","2669"],"pdf.docinfo.custom.book":"Advances in Neural Information Processing Systems 30","pdf.docinfo.custom.created":"2017","pdf.docinfo.custom.date":"2017","pdf.docinfo.custom.description":"Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)","pdf.docinfo.custom.description-abstract":"We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the “normalized” bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.","pdf.docinfo.custom.editors":"I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","pdf.docinfo.custom.eventtype":"Poster","pdf.docinfo.custom.published":"2017","pdf.docinfo.custom.publisher":"Curran Associates, Inc.","pdf.docinfo.custom.type":"Conference Proceedings","pdf.docinfo.custom.firstpage":"1923","pdf.docinfo.custom.lastpage":"1932","pdf.docinfo.modified":"2018-02-13T09:03:50Z","pdf.docinfo.producer":"PyPDF2","pdf.docinfo.subject":"Neural Information Processing Systems http://nips.cc/","pdf.encrypted":false,"pdf.hasmarkedcontent":false,"pdf.hasxfa":false,"pdf.hasxmp":false,"pdf.unmappedunicodecharsperpage":["19","0","1","15","3","4","0","0","0","180"],"producer":"PyPDF2","subject":"Neural Information Processing Systems http://nips.cc/"},"sha256":"9F3BB5D07EFB3758D60EF2120E4B203CCF134D0F121705B3A30BCA0860604DA6","clean.language":"en,en-us","clean.title":"revisit fuzzy neural network: demystifying batch normalization and relu with generalized hamming network","clean.pageCount":"10","clean.author":"lixin fan","clean.creator":"lixin fan"}